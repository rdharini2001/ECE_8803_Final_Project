{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "/storage/ice1/shared/d-pace_community/makerspace-datasets/MEDICAL/OLIVES"
      ],
      "metadata": {
        "id": "Gqkwy_UMieJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "data = np.genfromtxt('/content/Biomarker_Clinical_Data_Images.csv', delimiter=',', skip_header=1, usecols=(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20))\n",
        "\n",
        "with_headers = np.genfromtxt('/content/Biomarker_Clinical_Data_Images.csv', delimiter=',', names=True, usecols=(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20))\n",
        "\n",
        "avgArr = np.zeros([int(data.shape[0]/49), data.shape[1]])\n",
        "\n",
        "for i in range(0, int(data.shape[0]/49)):\n",
        "  start = 49*i+1\n",
        "  scans = data[start:start+48,:]\n",
        "  avgArr[i] = scans.mean(axis=0)\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "presence = np.ceil(avgArr)\n",
        "\n",
        "biomarkerAvg, temp = np.hsplit(avgArr, np.array([16]))\n",
        "__, scores = np.hsplit(temp, np.array([1]))\n",
        "biomarkerPresence, __ = np.hsplit(presence, np.array([16]))\n",
        "\n",
        "avg_train, avg_test, score_train, score_test = train_test_split(biomarkerAvg, scores, train_size = 150, test_size=42, random_state=0)\n",
        "presence_train, presence_test, __, __ = train_test_split(biomarkerPresence, scores, train_size = 150, test_size=42, random_state=0)\n",
        "\n",
        "score_train = (score_train - score_train.min(0))/score_train.ptp(0)\n",
        "score_test = (score_test - score_test.min(0))/score_test.ptp(0)\n",
        "\n",
        "np.savetxt('presence.csv', presence, delimiter=',')\n",
        "np.savetxt('average.csv', avgArr, delimiter=',')\n",
        "np.savetxt('scores.csv', scores, delimiter=',')\n"
      ],
      "metadata": {
        "id": "RDZ7l291AiNG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_avg_train = Variable(torch.from_numpy(avg_train).float())\n",
        "torch_avg_test = Variable(torch.from_numpy(avg_test).float())\n",
        "torch_presence_train = Variable(torch.from_numpy(presence_train).float())\n",
        "torch_presence_test = Variable(torch.from_numpy(presence_test).float())\n",
        "torch_score_train = Variable(torch.from_numpy(score_train).float())\n",
        "torch_score_test = Variable(torch.from_numpy(score_test).float())\n",
        "\n",
        "class FullLinearRegressionModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullLinearRegressionModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(2, 4)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(4, 16)\n",
        "    def forward(self, x):\n",
        "        X = self.linear(x)\n",
        "        X2 = self.relu(X)\n",
        "        y_pred = self.linear2(X2)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "m8e5z2fUFh6k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleLinearRegressionModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SingleLinearRegressionModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(2, 1)\n",
        "    def forward(self, x):\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "ou18SWFfsgrW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  del avgModel\n",
        "  del presenceModel\n",
        "  del loss_fn\n",
        "  del avg_optimizer\n",
        "  del presence_optimizer\n",
        "except:\n",
        "  pass\n",
        "\n",
        "presenceModel = FullLinearRegressionModel()\n",
        "avgModel = FullLinearRegressionModel()\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "avg_optimizer = torch.optim.SGD(avgModel.parameters(), lr = 0.01)\n",
        "presence_optimizer = torch.optim.SGD(presenceModel.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "hQUKx984bBWW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5000):\n",
        "\n",
        "    # Forward pass: Compute predicted y by passing\n",
        "    # x to the model\n",
        "    pred_y_presence = presenceModel(torch_score_train)\n",
        "    pred_y_avg = avgModel(torch_score_train)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss_presence = loss_fn(pred_y_presence, torch_presence_train)\n",
        "    loss_avg = loss_fn(pred_y_avg, torch_avg_train)\n",
        "\n",
        "    # Zero gradients, perform a backward pass,\n",
        "    # and update the weights.\n",
        "    avg_optimizer.zero_grad()\n",
        "    presence_optimizer.zero_grad()\n",
        "\n",
        "    loss_presence.backward()\n",
        "    loss_avg.backward()\n",
        "\n",
        "    avg_optimizer.step()\n",
        "    presence_optimizer.step()\n",
        "\n",
        "    if (epoch % 20 == 0):\n",
        "      print('epoch {:04}: loss (average):  {}'.format(epoch, loss_avg.item()))\n",
        "      print('            loss (presence): {}'.format(loss_presence.item()))\n",
        "\n",
        "test_pred_avg = avgModel(torch_score_test)\n",
        "test_pred_presence = presenceModel(torch_score_test)\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "torch.set_printoptions(threshold=10_000)\n",
        "#print(test_pred_avg)\n",
        "#print(test_pred_presence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qipi2VvuNUa-",
        "outputId": "79e17dcb-3554-4fc0-9f10-2a0aab9cfacc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0000: loss (average):  0.048526570200920105\n",
            "            loss (presence): 0.11410733312368393\n",
            "epoch 0020: loss (average):  0.04851962998509407\n",
            "            loss (presence): 0.11409852653741837\n",
            "epoch 0040: loss (average):  0.04851267859339714\n",
            "            loss (presence): 0.11408958584070206\n",
            "epoch 0060: loss (average):  0.04850572347640991\n",
            "            loss (presence): 0.11408066004514694\n",
            "epoch 0080: loss (average):  0.048498764634132385\n",
            "            loss (presence): 0.11407177895307541\n",
            "epoch 0100: loss (average):  0.048491813242435455\n",
            "            loss (presence): 0.11406290531158447\n",
            "epoch 0120: loss (average):  0.04848484694957733\n",
            "            loss (presence): 0.11405405402183533\n",
            "epoch 0140: loss (average):  0.04847788065671921\n",
            "            loss (presence): 0.11404523253440857\n",
            "epoch 0160: loss (average):  0.048470910638570786\n",
            "            loss (presence): 0.11403641104698181\n",
            "epoch 0180: loss (average):  0.048463933169841766\n",
            "            loss (presence): 0.11402761936187744\n",
            "epoch 0200: loss (average):  0.04845694825053215\n",
            "            loss (presence): 0.11401883512735367\n",
            "epoch 0220: loss (average):  0.04844997078180313\n",
            "            loss (presence): 0.11401009559631348\n",
            "epoch 0240: loss (average):  0.04844298213720322\n",
            "            loss (presence): 0.11400136351585388\n",
            "epoch 0260: loss (average):  0.048435989767313004\n",
            "            loss (presence): 0.11399265378713608\n",
            "epoch 0280: loss (average):  0.04842900112271309\n",
            "            loss (presence): 0.11398395895957947\n",
            "epoch 0300: loss (average):  0.04842200130224228\n",
            "            loss (presence): 0.11397528648376465\n",
            "epoch 0320: loss (average):  0.04841500520706177\n",
            "            loss (presence): 0.11396663635969162\n",
            "epoch 0340: loss (average):  0.04840799793601036\n",
            "            loss (presence): 0.11395799368619919\n",
            "epoch 0360: loss (average):  0.04840098321437836\n",
            "            loss (presence): 0.11394936591386795\n",
            "epoch 0380: loss (average):  0.04839397594332695\n",
            "            loss (presence): 0.1139407828450203\n",
            "epoch 0400: loss (average):  0.048386961221694946\n",
            "            loss (presence): 0.11393219977617264\n",
            "epoch 0420: loss (average):  0.04837995022535324\n",
            "            loss (presence): 0.11392364650964737\n",
            "epoch 0440: loss (average):  0.04837292060256004\n",
            "            loss (presence): 0.11391511559486389\n",
            "epoch 0460: loss (average):  0.04836589843034744\n",
            "            loss (presence): 0.11390657722949982\n",
            "epoch 0480: loss (average):  0.04835886508226395\n",
            "            loss (presence): 0.11389808356761932\n",
            "epoch 0500: loss (average):  0.04835182800889015\n",
            "            loss (presence): 0.11388960480690002\n",
            "epoch 0520: loss (average):  0.04834479093551636\n",
            "            loss (presence): 0.11388114839792252\n",
            "epoch 0540: loss (average):  0.04833775386214256\n",
            "            loss (presence): 0.1138727068901062\n",
            "epoch 0560: loss (average):  0.04833071306347847\n",
            "            loss (presence): 0.1138644814491272\n",
            "epoch 0580: loss (average):  0.04832366481423378\n",
            "            loss (presence): 0.11385626345872879\n",
            "epoch 0600: loss (average):  0.04831661656498909\n",
            "            loss (presence): 0.11384805291891098\n",
            "epoch 0620: loss (average):  0.0483095645904541\n",
            "            loss (presence): 0.11383987218141556\n",
            "epoch 0640: loss (average):  0.048302508890628815\n",
            "            loss (presence): 0.11383169889450073\n",
            "epoch 0660: loss (average):  0.04829544201493263\n",
            "            loss (presence): 0.1138235479593277\n",
            "epoch 0680: loss (average):  0.04828837886452675\n",
            "            loss (presence): 0.11381541937589645\n",
            "epoch 0700: loss (average):  0.048281311988830566\n",
            "            loss (presence): 0.1138072982430458\n",
            "epoch 0720: loss (average):  0.048274241387844086\n",
            "            loss (presence): 0.11379919946193695\n",
            "epoch 0740: loss (average):  0.048267170786857605\n",
            "            loss (presence): 0.1137911006808281\n",
            "epoch 0760: loss (average):  0.04826009273529053\n",
            "            loss (presence): 0.11378303170204163\n",
            "epoch 0780: loss (average):  0.04825301468372345\n",
            "            loss (presence): 0.11377444118261337\n",
            "epoch 0800: loss (average):  0.04824592545628548\n",
            "            loss (presence): 0.11376551538705826\n",
            "epoch 0820: loss (average):  0.048238836228847504\n",
            "            loss (presence): 0.11375663429498672\n",
            "epoch 0840: loss (average):  0.04823175072669983\n",
            "            loss (presence): 0.11374776065349579\n",
            "epoch 0860: loss (average):  0.04822465404868126\n",
            "            loss (presence): 0.11373890936374664\n",
            "epoch 0880: loss (average):  0.04821756109595299\n",
            "            loss (presence): 0.11373008787631989\n",
            "epoch 0900: loss (average):  0.04821046069264412\n",
            "            loss (presence): 0.11372130364179611\n",
            "epoch 0920: loss (average):  0.04820336028933525\n",
            "            loss (presence): 0.11371254175901413\n",
            "epoch 0940: loss (average):  0.048196252435445786\n",
            "            loss (presence): 0.11370379477739334\n",
            "epoch 0960: loss (average):  0.04818914458155632\n",
            "            loss (presence): 0.11369507014751434\n",
            "epoch 0980: loss (average):  0.04818202927708626\n",
            "            loss (presence): 0.11368638277053833\n",
            "epoch 1000: loss (average):  0.048174913972616196\n",
            "            loss (presence): 0.11367771029472351\n",
            "epoch 1020: loss (average):  0.048167791217565536\n",
            "            loss (presence): 0.11366906762123108\n",
            "epoch 1040: loss (average):  0.048160672187805176\n",
            "            loss (presence): 0.11366045475006104\n",
            "epoch 1060: loss (average):  0.04815354943275452\n",
            "            loss (presence): 0.11365184932947159\n",
            "epoch 1080: loss (average):  0.04814642295241356\n",
            "            loss (presence): 0.11364328116178513\n",
            "epoch 1100: loss (average):  0.0481392927467823\n",
            "            loss (presence): 0.11363472044467926\n",
            "epoch 1120: loss (average):  0.04813215881586075\n",
            "            loss (presence): 0.11362619698047638\n",
            "epoch 1140: loss (average):  0.048125024884939194\n",
            "            loss (presence): 0.11361769586801529\n",
            "epoch 1160: loss (average):  0.04811788350343704\n",
            "            loss (presence): 0.1136092022061348\n",
            "epoch 1180: loss (average):  0.04811074212193489\n",
            "            loss (presence): 0.11360075324773788\n",
            "epoch 1200: loss (average):  0.04810359328985214\n",
            "            loss (presence): 0.11359231173992157\n",
            "epoch 1220: loss (average):  0.04809644818305969\n",
            "            loss (presence): 0.11358389258384705\n",
            "epoch 1240: loss (average):  0.048089299350976944\n",
            "            loss (presence): 0.11357550323009491\n",
            "epoch 1260: loss (average):  0.0480821430683136\n",
            "            loss (presence): 0.11356712132692337\n",
            "epoch 1280: loss (average):  0.048074983060359955\n",
            "            loss (presence): 0.11355878412723541\n",
            "epoch 1300: loss (average):  0.04806782677769661\n",
            "            loss (presence): 0.11355045437812805\n",
            "epoch 1320: loss (average):  0.04806066304445267\n",
            "            loss (presence): 0.11354214698076248\n",
            "epoch 1340: loss (average):  0.04805350303649902\n",
            "            loss (presence): 0.1135338693857193\n",
            "epoch 1360: loss (average):  0.04804633557796478\n",
            "            loss (presence): 0.11352560669183731\n",
            "epoch 1380: loss (average):  0.04803916811943054\n",
            "            loss (presence): 0.11351736634969711\n",
            "epoch 1400: loss (average):  0.048031996935606\n",
            "            loss (presence): 0.1135091558098793\n",
            "epoch 1420: loss (average):  0.04802481830120087\n",
            "            loss (presence): 0.11350095272064209\n",
            "epoch 1440: loss (average):  0.04801764339208603\n",
            "            loss (presence): 0.11349277198314667\n",
            "epoch 1460: loss (average):  0.048010461032390594\n",
            "            loss (presence): 0.11348461359739304\n",
            "epoch 1480: loss (average):  0.04800327867269516\n",
            "            loss (presence): 0.11347641050815582\n",
            "epoch 1500: loss (average):  0.04799608886241913\n",
            "            loss (presence): 0.1134682223200798\n",
            "epoch 1520: loss (average):  0.047988902777433395\n",
            "            loss (presence): 0.11346007138490677\n",
            "epoch 1540: loss (average):  0.047981709241867065\n",
            "            loss (presence): 0.11345192044973373\n",
            "epoch 1560: loss (average):  0.047974519431591034\n",
            "            loss (presence): 0.11344380676746368\n",
            "epoch 1580: loss (average):  0.04796731844544411\n",
            "            loss (presence): 0.11343573033809662\n",
            "epoch 1600: loss (average):  0.04796011745929718\n",
            "            loss (presence): 0.11342766135931015\n",
            "epoch 1620: loss (average):  0.04795292392373085\n",
            "            loss (presence): 0.11341960728168488\n",
            "epoch 1640: loss (average):  0.04794572293758392\n",
            "            loss (presence): 0.1134115606546402\n",
            "epoch 1660: loss (average):  0.0479385182261467\n",
            "            loss (presence): 0.1134035736322403\n",
            "epoch 1680: loss (average):  0.04793131351470947\n",
            "            loss (presence): 0.1133955791592598\n",
            "epoch 1700: loss (average):  0.04792410135269165\n",
            "            loss (presence): 0.11338761448860168\n",
            "epoch 1720: loss (average):  0.04791688919067383\n",
            "            loss (presence): 0.11337967216968536\n",
            "epoch 1740: loss (average):  0.04790967330336571\n",
            "            loss (presence): 0.11337175965309143\n",
            "epoch 1760: loss (average):  0.047902464866638184\n",
            "            loss (presence): 0.11336386203765869\n",
            "epoch 1780: loss (average):  0.047895241528749466\n",
            "            loss (presence): 0.11335596442222595\n",
            "epoch 1800: loss (average):  0.04788802191615105\n",
            "            loss (presence): 0.11334811896085739\n",
            "epoch 1820: loss (average):  0.04788080230355263\n",
            "            loss (presence): 0.11334028840065002\n",
            "epoch 1840: loss (average):  0.04787357524037361\n",
            "            loss (presence): 0.11333245784044266\n",
            "epoch 1860: loss (average):  0.047866351902484894\n",
            "            loss (presence): 0.11332466453313828\n",
            "epoch 1880: loss (average):  0.04785912111401558\n",
            "            loss (presence): 0.11331689357757568\n",
            "epoch 1900: loss (average):  0.047851890325546265\n",
            "            loss (presence): 0.11330913752317429\n",
            "epoch 1920: loss (average):  0.04784465953707695\n",
            "            loss (presence): 0.11330138891935349\n",
            "epoch 1940: loss (average):  0.04783742129802704\n",
            "            loss (presence): 0.11329368501901627\n",
            "epoch 1960: loss (average):  0.04783017933368683\n",
            "            loss (presence): 0.11328598111867905\n",
            "epoch 1980: loss (average):  0.04782294109463692\n",
            "            loss (presence): 0.11327831447124481\n",
            "epoch 2000: loss (average):  0.04781569913029671\n",
            "            loss (presence): 0.11327067017555237\n",
            "epoch 2020: loss (average):  0.0478084534406662\n",
            "            loss (presence): 0.11326304078102112\n",
            "epoch 2040: loss (average):  0.04780120775103569\n",
            "            loss (presence): 0.11325542628765106\n",
            "epoch 2060: loss (average):  0.04779396206140518\n",
            "            loss (presence): 0.1132478192448616\n",
            "epoch 2080: loss (average):  0.04778671637177467\n",
            "            loss (presence): 0.11324025690555573\n",
            "epoch 2100: loss (average):  0.04777945950627327\n",
            "            loss (presence): 0.11323271691799164\n",
            "epoch 2120: loss (average):  0.047772206366062164\n",
            "            loss (presence): 0.11322514712810516\n",
            "epoch 2140: loss (average):  0.04776495695114136\n",
            "            loss (presence): 0.11321753263473511\n",
            "epoch 2160: loss (average):  0.047757700085639954\n",
            "            loss (presence): 0.11320995539426804\n",
            "epoch 2180: loss (average):  0.04775043949484825\n",
            "            loss (presence): 0.11320237815380096\n",
            "epoch 2200: loss (average):  0.04774317890405655\n",
            "            loss (presence): 0.11319482326507568\n",
            "epoch 2220: loss (average):  0.04773591831326485\n",
            "            loss (presence): 0.1131872907280922\n",
            "epoch 2240: loss (average):  0.04772866144776344\n",
            "            loss (presence): 0.1131797805428505\n",
            "epoch 2260: loss (average):  0.04772139713168144\n",
            "            loss (presence): 0.11317230015993118\n",
            "epoch 2280: loss (average):  0.04771413654088974\n",
            "            loss (presence): 0.11316482722759247\n",
            "epoch 2300: loss (average):  0.04770686849951744\n",
            "            loss (presence): 0.11315738409757614\n",
            "epoch 2320: loss (average):  0.04769960418343544\n",
            "            loss (presence): 0.1131499707698822\n",
            "epoch 2340: loss (average):  0.04769233241677284\n",
            "            loss (presence): 0.11314255744218826\n",
            "epoch 2360: loss (average):  0.04768506437540054\n",
            "            loss (presence): 0.11313517391681671\n",
            "epoch 2380: loss (average):  0.04767778888344765\n",
            "            loss (presence): 0.11312779784202576\n",
            "epoch 2400: loss (average):  0.04767051711678505\n",
            "            loss (presence): 0.11312045902013779\n",
            "epoch 2420: loss (average):  0.04766324535012245\n",
            "            loss (presence): 0.11311325430870056\n",
            "epoch 2440: loss (average):  0.047655973583459854\n",
            "            loss (presence): 0.11310610175132751\n",
            "epoch 2460: loss (average):  0.04764869436621666\n",
            "            loss (presence): 0.11309897154569626\n",
            "epoch 2480: loss (average):  0.04764141887426376\n",
            "            loss (presence): 0.1130918636918068\n",
            "epoch 2500: loss (average):  0.04763413593173027\n",
            "            loss (presence): 0.11308475583791733\n",
            "epoch 2520: loss (average):  0.04762685298919678\n",
            "            loss (presence): 0.11307768523693085\n",
            "epoch 2540: loss (average):  0.04761957377195358\n",
            "            loss (presence): 0.11307062953710556\n",
            "epoch 2560: loss (average):  0.04761229082942009\n",
            "            loss (presence): 0.11306359618902206\n",
            "epoch 2580: loss (average):  0.0476050078868866\n",
            "            loss (presence): 0.11305656284093857\n",
            "epoch 2600: loss (average):  0.047597721219062805\n",
            "            loss (presence): 0.11304957419633865\n",
            "epoch 2620: loss (average):  0.047590430825948715\n",
            "            loss (presence): 0.11304257810115814\n",
            "epoch 2640: loss (average):  0.04758313670754433\n",
            "            loss (presence): 0.11303561925888062\n",
            "epoch 2660: loss (average):  0.047575853765010834\n",
            "            loss (presence): 0.11302866786718369\n",
            "epoch 2680: loss (average):  0.04756857082247734\n",
            "            loss (presence): 0.11302173882722855\n",
            "epoch 2700: loss (average):  0.047561272978782654\n",
            "            loss (presence): 0.113014817237854\n",
            "epoch 2720: loss (average):  0.04755398631095886\n",
            "            loss (presence): 0.11300795525312424\n",
            "epoch 2740: loss (average):  0.047546692192554474\n",
            "            loss (presence): 0.11300107091665268\n",
            "epoch 2760: loss (average):  0.047539401799440384\n",
            "            loss (presence): 0.1129942312836647\n",
            "epoch 2780: loss (average):  0.047532111406326294\n",
            "            loss (presence): 0.11298739165067673\n",
            "epoch 2800: loss (average):  0.04752481356263161\n",
            "            loss (presence): 0.11298057436943054\n",
            "epoch 2820: loss (average):  0.04751751571893692\n",
            "            loss (presence): 0.11297377198934555\n",
            "epoch 2840: loss (average):  0.04751022160053253\n",
            "            loss (presence): 0.11296699196100235\n",
            "epoch 2860: loss (average):  0.047502923756837845\n",
            "            loss (presence): 0.11296022683382034\n",
            "epoch 2880: loss (average):  0.04749562591314316\n",
            "            loss (presence): 0.11295349150896072\n",
            "epoch 2900: loss (average):  0.04748832806944847\n",
            "            loss (presence): 0.11294674873352051\n",
            "epoch 2920: loss (average):  0.047481026500463486\n",
            "            loss (presence): 0.11294006556272507\n",
            "epoch 2940: loss (average):  0.0474737286567688\n",
            "            loss (presence): 0.11293336004018784\n",
            "epoch 2960: loss (average):  0.04746643453836441\n",
            "            loss (presence): 0.11292668431997299\n",
            "epoch 2980: loss (average):  0.047459136694669724\n",
            "            loss (presence): 0.11292003840208054\n",
            "epoch 3000: loss (average):  0.04745183140039444\n",
            "            loss (presence): 0.11291341483592987\n",
            "epoch 3020: loss (average):  0.047444529831409454\n",
            "            loss (presence): 0.11290678381919861\n",
            "epoch 3040: loss (average):  0.04743722826242447\n",
            "            loss (presence): 0.11290019005537033\n",
            "epoch 3060: loss (average):  0.047429922968149185\n",
            "            loss (presence): 0.11289361119270325\n",
            "epoch 3080: loss (average):  0.0474226288497448\n",
            "            loss (presence): 0.11288703978061676\n",
            "epoch 3100: loss (average):  0.047415319830179214\n",
            "            loss (presence): 0.11288049072027206\n",
            "epoch 3120: loss (average):  0.04740802198648453\n",
            "            loss (presence): 0.11287396401166916\n",
            "epoch 3140: loss (average):  0.047400716692209244\n",
            "            loss (presence): 0.11286745965480804\n",
            "epoch 3160: loss (average):  0.04739341139793396\n",
            "            loss (presence): 0.11286097019910812\n",
            "epoch 3180: loss (average):  0.047386109828948975\n",
            "            loss (presence): 0.11285450309515\n",
            "epoch 3200: loss (average):  0.04737880825996399\n",
            "            loss (presence): 0.11284801363945007\n",
            "epoch 3220: loss (average):  0.047371502965688705\n",
            "            loss (presence): 0.11284136772155762\n",
            "epoch 3240: loss (average):  0.04736420139670372\n",
            "            loss (presence): 0.11283473670482635\n",
            "epoch 3260: loss (average):  0.047356896102428436\n",
            "            loss (presence): 0.11282812803983688\n",
            "epoch 3280: loss (average):  0.04734959453344345\n",
            "            loss (presence): 0.11282152682542801\n",
            "epoch 3300: loss (average):  0.047342292964458466\n",
            "            loss (presence): 0.11281495541334152\n",
            "epoch 3320: loss (average):  0.04733498767018318\n",
            "            loss (presence): 0.11280840635299683\n",
            "epoch 3340: loss (average):  0.047327689826488495\n",
            "            loss (presence): 0.11280185729265213\n",
            "epoch 3360: loss (average):  0.04732038080692291\n",
            "            loss (presence): 0.11279527097940445\n",
            "epoch 3380: loss (average):  0.047313082963228226\n",
            "            loss (presence): 0.11278858035802841\n",
            "epoch 3400: loss (average):  0.04730577766895294\n",
            "            loss (presence): 0.11278188973665237\n",
            "epoch 3420: loss (average):  0.04729847237467766\n",
            "            loss (presence): 0.11277524381875992\n",
            "epoch 3440: loss (average):  0.04729117080569267\n",
            "            loss (presence): 0.11276860535144806\n",
            "epoch 3460: loss (average):  0.047283872961997986\n",
            "            loss (presence): 0.11276199668645859\n",
            "epoch 3480: loss (average):  0.0472765676677227\n",
            "            loss (presence): 0.11275541037321091\n",
            "epoch 3500: loss (average):  0.047269269824028015\n",
            "            loss (presence): 0.11274883151054382\n",
            "epoch 3520: loss (average):  0.04726196825504303\n",
            "            loss (presence): 0.11274224519729614\n",
            "epoch 3540: loss (average):  0.047254666686058044\n",
            "            loss (presence): 0.11273572593927383\n",
            "epoch 3560: loss (average):  0.04724736884236336\n",
            "            loss (presence): 0.11272918432950974\n",
            "epoch 3580: loss (average):  0.047240063548088074\n",
            "            loss (presence): 0.11272268742322922\n",
            "epoch 3600: loss (average):  0.047232769429683685\n",
            "            loss (presence): 0.11271604150533676\n",
            "epoch 3620: loss (average):  0.0472254678606987\n",
            "            loss (presence): 0.11270900815725327\n",
            "epoch 3640: loss (average):  0.04721817746758461\n",
            "            loss (presence): 0.11270192265510559\n",
            "epoch 3660: loss (average):  0.047210875898599625\n",
            "            loss (presence): 0.1126943975687027\n",
            "epoch 3680: loss (average):  0.04720357432961464\n",
            "            loss (presence): 0.11268624663352966\n",
            "epoch 3700: loss (average):  0.04719628021121025\n",
            "            loss (presence): 0.11267813295125961\n",
            "epoch 3720: loss (average):  0.04718898609280586\n",
            "            loss (presence): 0.11267005652189255\n",
            "epoch 3740: loss (average):  0.047181688249111176\n",
            "            loss (presence): 0.11266202479600906\n",
            "epoch 3760: loss (average):  0.04717439413070679\n",
            "            loss (presence): 0.11265403777360916\n",
            "epoch 3780: loss (average):  0.0471671037375927\n",
            "            loss (presence): 0.11264608055353165\n",
            "epoch 3800: loss (average):  0.04715981334447861\n",
            "            loss (presence): 0.11263815313577652\n",
            "epoch 3820: loss (average):  0.04715252295136452\n",
            "            loss (presence): 0.11263028532266617\n",
            "epoch 3840: loss (average):  0.04714523255825043\n",
            "            loss (presence): 0.11262241005897522\n",
            "epoch 3860: loss (average):  0.04713794216513634\n",
            "            loss (presence): 0.11261460930109024\n",
            "epoch 3880: loss (average):  0.04713064804673195\n",
            "            loss (presence): 0.11260683834552765\n",
            "epoch 3900: loss (average):  0.04712336137890816\n",
            "            loss (presence): 0.11259908229112625\n",
            "epoch 3920: loss (average):  0.04711607098579407\n",
            "            loss (presence): 0.11259137839078903\n",
            "epoch 3940: loss (average):  0.04710879176855087\n",
            "            loss (presence): 0.1125836968421936\n",
            "epoch 3960: loss (average):  0.04710150510072708\n",
            "            loss (presence): 0.11257605254650116\n",
            "epoch 3980: loss (average):  0.04709421470761299\n",
            "            loss (presence): 0.11256878077983856\n",
            "epoch 4000: loss (average):  0.0470869354903698\n",
            "            loss (presence): 0.11256156861782074\n",
            "epoch 4020: loss (average):  0.0470796562731266\n",
            "            loss (presence): 0.1125539019703865\n",
            "epoch 4040: loss (average):  0.04707237705588341\n",
            "            loss (presence): 0.11254604160785675\n",
            "epoch 4060: loss (average):  0.04706510156393051\n",
            "            loss (presence): 0.11253824830055237\n",
            "epoch 4080: loss (average):  0.04705782234668732\n",
            "            loss (presence): 0.11253045499324799\n",
            "epoch 4100: loss (average):  0.04705054685473442\n",
            "            loss (presence): 0.11252271384000778\n",
            "epoch 4120: loss (average):  0.047043267637491226\n",
            "            loss (presence): 0.11251481622457504\n",
            "epoch 4140: loss (average):  0.04703599959611893\n",
            "            loss (presence): 0.11250630766153336\n",
            "epoch 4160: loss (average):  0.04702872410416603\n",
            "            loss (presence): 0.11249782890081406\n",
            "epoch 4180: loss (average):  0.04702145978808403\n",
            "            loss (presence): 0.11248940974473953\n",
            "epoch 4200: loss (average):  0.047014184296131134\n",
            "            loss (presence): 0.11248104274272919\n",
            "epoch 4220: loss (average):  0.047006916254758835\n",
            "            loss (presence): 0.11247271299362183\n",
            "epoch 4240: loss (average):  0.046999648213386536\n",
            "            loss (presence): 0.11246440559625626\n",
            "epoch 4260: loss (average):  0.046992383897304535\n",
            "            loss (presence): 0.11245615780353546\n",
            "epoch 4280: loss (average):  0.04698512330651283\n",
            "            loss (presence): 0.11244795471429825\n",
            "epoch 4300: loss (average):  0.04697786271572113\n",
            "            loss (presence): 0.11243976652622223\n",
            "epoch 4320: loss (average):  0.04697060212492943\n",
            "            loss (presence): 0.11243163794279099\n",
            "epoch 4340: loss (average):  0.046963345259428024\n",
            "            loss (presence): 0.11242355406284332\n",
            "epoch 4360: loss (average):  0.04695609211921692\n",
            "            loss (presence): 0.11241549253463745\n",
            "epoch 4380: loss (average):  0.046948838979005814\n",
            "            loss (presence): 0.11240699887275696\n",
            "epoch 4400: loss (average):  0.04694158956408501\n",
            "            loss (presence): 0.11239850521087646\n",
            "epoch 4420: loss (average):  0.0469343364238739\n",
            "            loss (presence): 0.11239007115364075\n",
            "epoch 4440: loss (average):  0.046927087008953094\n",
            "            loss (presence): 0.11238165199756622\n",
            "epoch 4460: loss (average):  0.046919845044612885\n",
            "            loss (presence): 0.11237329989671707\n",
            "epoch 4480: loss (average):  0.046912599354982376\n",
            "            loss (presence): 0.1123649999499321\n",
            "epoch 4500: loss (average):  0.04690535366535187\n",
            "            loss (presence): 0.1123562753200531\n",
            "epoch 4520: loss (average):  0.046898115426301956\n",
            "            loss (presence): 0.11234734952449799\n",
            "epoch 4540: loss (average):  0.046890877187252045\n",
            "            loss (presence): 0.11233829706907272\n",
            "epoch 4560: loss (average):  0.04688364267349243\n",
            "            loss (presence): 0.11232928931713104\n",
            "epoch 4580: loss (average):  0.04687640815973282\n",
            "            loss (presence): 0.11232034116983414\n",
            "epoch 4600: loss (average):  0.046869173645973206\n",
            "            loss (presence): 0.1123112365603447\n",
            "epoch 4620: loss (average):  0.04686194285750389\n",
            "            loss (presence): 0.11230263859033585\n",
            "epoch 4640: loss (average):  0.046854715794324875\n",
            "            loss (presence): 0.1122935488820076\n",
            "epoch 4660: loss (average):  0.04684748500585556\n",
            "            loss (presence): 0.1122833788394928\n",
            "epoch 4680: loss (average):  0.04684026166796684\n",
            "            loss (presence): 0.11227258294820786\n",
            "epoch 4700: loss (average):  0.046833038330078125\n",
            "            loss (presence): 0.11226153373718262\n",
            "epoch 4720: loss (average):  0.046825818717479706\n",
            "            loss (presence): 0.11225054413080215\n",
            "epoch 4740: loss (average):  0.046818606555461884\n",
            "            loss (presence): 0.11223963648080826\n",
            "epoch 4760: loss (average):  0.04681139066815376\n",
            "            loss (presence): 0.11222881078720093\n",
            "epoch 4780: loss (average):  0.04680417850613594\n",
            "            loss (presence): 0.112217977643013\n",
            "epoch 4800: loss (average):  0.04679696634411812\n",
            "            loss (presence): 0.11220703274011612\n",
            "epoch 4820: loss (average):  0.046789757907390594\n",
            "            loss (presence): 0.11219614744186401\n",
            "epoch 4840: loss (average):  0.04678255319595337\n",
            "            loss (presence): 0.11218535155057907\n",
            "epoch 4860: loss (average):  0.04677535220980644\n",
            "            loss (presence): 0.11217482388019562\n",
            "epoch 4880: loss (average):  0.046768154948949814\n",
            "            loss (presence): 0.11216457188129425\n",
            "epoch 4900: loss (average):  0.046760957688093185\n",
            "            loss (presence): 0.11215441673994064\n",
            "epoch 4920: loss (average):  0.046753764152526855\n",
            "            loss (presence): 0.11214429140090942\n",
            "epoch 4940: loss (average):  0.04674656689167023\n",
            "            loss (presence): 0.11213424801826477\n",
            "epoch 4960: loss (average):  0.046739380806684494\n",
            "            loss (presence): 0.11212426424026489\n",
            "epoch 4980: loss (average):  0.04673219472169876\n",
            "            loss (presence): 0.11211434751749039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "for i in range(0,16):\n",
        "  del singleModel\n",
        "  biomarker = biomarkerPresence[:,i].reshape(-1,1)\n",
        "\n",
        "  bio_train, bio_test, __, __ = train_test_split(biomarker, scores, train_size = 150, test_size=42, random_state=0)\n",
        "\n",
        "  torch_bio_train = Variable(torch.from_numpy(bio_train).float())\n",
        "  torch_bio_test = Variable(torch.from_numpy(bio_test).float())\n",
        "\n",
        "  singleModel = SingleLinearRegressionModel()\n",
        "  loss_fn = torch.nn.MSELoss()\n",
        "  optimizer = torch.optim.SGD(singleModel.parameters(), lr = 0.01)\n",
        "  for epoch in range(100):\n",
        "    pred_y = singleModel(torch_score_train)\n",
        "    loss = loss_fn(pred_y, torch_bio_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #if (epoch % 50 == 0):\n",
        "    #  print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "  pred = singleModel(torch_score_test)\n",
        "  pred = pred > 0.5\n",
        "  loss = loss_fn(pred, torch_bio_test)\n",
        "  f1 = f1_score(torch_bio_test, pred, average='binary', zero_division=np.nan)\n",
        "  #print(f\"{with_headers.dtype.names[i]}: {f1}\")\n",
        "  print(f\"{i+1}: {f1}\")\n",
        "  #if i == 8:\n",
        "  #  print(pred)\n",
        "  #  print(torch_bio_test)\n",
        "  #print(f\"{i+1}: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNakc_-O7dOJ",
        "outputId": "021aeb2f-deed-41a3-c72b-c66d0fc9c0be"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 0.0\n",
            "2: 0.0\n",
            "3: 0.0\n",
            "4: 0.0\n",
            "5: 1.0\n",
            "6: 0.631578947368421\n",
            "7: 0.7\n",
            "8: 0.0\n",
            "9: 0.935064935064935\n",
            "10: 0.0\n",
            "11: 0.6774193548387096\n",
            "12: 0.9230769230769231\n",
            "13: 0.3333333333333333\n",
            "14: nan\n",
            "15: nan\n",
            "16: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.75:\n",
        "\n",
        "1: 0.0\n",
        "2: 0.0\n",
        "3: 0.0\n",
        "4: 0.0\n",
        "5: 1.0\n",
        "6: 0.0\n",
        "7: 0.2857142857142857\n",
        "8: 0.0\n",
        "9: 0.8985507246376812\n",
        "10: 0.0\n",
        "11: 0.0\n",
        "12: 0.782608695652174\n",
        "13: 0.0\n",
        "14: nan\n",
        "15: nan\n",
        "16: 0.0\n",
        "\n",
        "0.5:\n",
        "\n",
        "1: 0.0\n",
        "2: 0.0\n",
        "3: 0.0\n",
        "4: 0.125\n",
        "5: 1.0\n",
        "6: 0.6551724137931034\n",
        "7: 0.746268656716418\n",
        "8: 0.0\n",
        "9: 0.935064935064935\n",
        "10: 0.0\n",
        "11: 0.68\n",
        "12: 0.9230769230769231\n",
        "13: 0.0\n",
        "14: nan\n",
        "15: nan\n",
        "16: 0.0\n",
        "\n",
        "\n",
        "0.25:\n",
        "\n",
        "1: 0.0\n",
        "2: 0.4\n",
        "3: 0.0\n",
        "4: 0.4444444444444444\n",
        "5: 1.0\n",
        "6: 0.746268656716418\n",
        "7: 0.746268656716418\n",
        "8: 0.3333333333333333\n",
        "9: 0.9367088607594937\n",
        "10: 0.0\n",
        "11: 0.7076923076923077\n",
        "12: 0.9230769230769231\n",
        "13: 0.625\n",
        "14: nan\n",
        "15: 0.0\n",
        "16: 0.6\n",
        "\n",
        "0.05:\n",
        "\n",
        "1: 0.21052631578947367\n",
        "2: 0.38461538461538464\n",
        "3: 0.2222222222222222\n",
        "4: 0.4444444444444444\n",
        "5: 1.0\n",
        "6: 0.7647058823529411\n",
        "7: 0.746268656716418\n",
        "8: 0.41509433962264153\n",
        "9: 0.95\n",
        "10: 0.0\n",
        "11: 0.7076923076923077\n",
        "12: 0.9230769230769231\n",
        "13: 0.10810810810810811\n",
        "14: 0.0\n",
        "15: 0.0\n",
        "16: 0.16216216216216217"
      ],
      "metadata": {
        "id": "DDPgGqn9I5hT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}